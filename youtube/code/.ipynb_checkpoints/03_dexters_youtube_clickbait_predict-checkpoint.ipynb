{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1543525896775,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "YhCXa93yFK4c",
    "outputId": "b0989fa7-c74e-447f-9fa0-b22f139367bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8434,
     "status": "ok",
     "timestamp": 1543525904909,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "cDs49m_6Gbu0",
    "outputId": "8d8e5406-63a3-418b-dcff-2cd883c32848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argparse in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Collecting re\n",
      "\u001b[31m  Could not find a version that satisfies the requirement re (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for re\u001b[0m\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Collecting pickle\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pickle (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pickle\u001b[0m\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
      "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.56)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.56 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.56)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.56->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.56->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install argparse\n",
    "!pip install re\n",
    "!pip install emoji\n",
    "!pip install pickle\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9ZKVFNIGcsk"
   },
   "outputs": [],
   "source": [
    "#for parsing\n",
    "import argparse\n",
    "\n",
    "#for regex\n",
    "import re\n",
    "\n",
    "#The entire set of Emoji codes as defined \n",
    "import emoji\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#erializing and de-serializing python object structure\n",
    "import pickle\n",
    "\n",
    "#topic modelling, document indexing and similarity retrieval with large corpora\n",
    "#Target audience is being natural language processing\n",
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzxySrcjQAH5"
   },
   "outputs": [],
   "source": [
    "#Created Models being Imported\n",
    "\n",
    "#word2vec\n",
    "#mean_title_embedding\n",
    "#mean_log_video_views\n",
    "#mean_log_video_likes\n",
    "#mean_log_video_dislikes\n",
    "#mean_log_video_comments\n",
    "#min_max_scaler\n",
    "\n",
    "#Prediction Model Used to predict the Result---------\n",
    "#svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8544,
     "status": "ok",
     "timestamp": 1543525905043,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "OcW_vD52Gexd",
    "outputId": "de32fb48-5303-44eb-82ac-3518a5073461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:64: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.19.1 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SVC from version 0.19.1 when using version 0.19.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#method for tokenization a string in python\n",
    "\n",
    "def tokenize(string):\n",
    "\n",
    "    \"\"\" Tokenizes a string.\n",
    "\n",
    "    Returns a list of stems and, eventually, emojis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on the Ranks NL (Google) stopwords list, but \"how\" and \"will\" are not stripped, and\n",
    "    # words shorter than 2 characters are not checked (since they are stripped):\n",
    "    \n",
    "    stop_words = [\n",
    "        \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"com\", \"for\", \"from\", \"in\", \"is\", \"it\", \"of\",\n",
    "        \"on\", \"or\", \"that\", \"the\", \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"with\",\n",
    "        \"the\", \"www\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    #Stripping the  whitespaces and alphanumerics\n",
    "    string = strip_short(\n",
    "        strip_multiple_whitespaces(\n",
    "            strip_punctuation(\n",
    "                split_alphanum(string))),\n",
    "        minsize=2)\n",
    "    \n",
    "    # Parse emojis:\n",
    "    emojis = [c for c in string if c in emoji.UNICODE_EMOJI]\n",
    "    \n",
    "    # Remove every non-word character and stem each word:\n",
    "    string = stem_text(re.sub(r\"[^\\w\\s,]\", \"\", string))\n",
    "    \n",
    "    # List of stems and emojis:\n",
    "    tokens = string.split() + emojis\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "        try:\n",
    "            tokens.remove(stop_word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def average_embedding(tokens, word2vec, na_vector=None):\n",
    "\n",
    "    \"\"\" Embeds a title with the average representation of its tokens.\n",
    "\n",
    "    Returns the mean vector representation of the tokens representations. When no token is in the\n",
    "    \"\"\"\n",
    "\n",
    "    vectors = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in word2vec:\n",
    "            vectors.append(word2vec[token])\n",
    "\n",
    "    if len(vectors) == 0 and na_vector is not None:\n",
    "        vectors.append(na_vector)\n",
    "\n",
    "    return np.mean(np.array(vectors), axis=0)\n",
    "\n",
    "\n",
    "# Import the Word2Vec model and the mean vector representation computed on the train set:\n",
    "word2vec = pickle.load(open(\"/content/drive/My Drive/youtube/data/word2vec\", \"rb\"))\n",
    "mean_title_embedding = pickle.load(open(\"/content/drive/My Drive/youtube/data/mean-title-embedding\", \"rb\"))\n",
    "\n",
    "#User input here:\n",
    "input = {\n",
    "    \"video_title\": \"New York\",\n",
    "    \"video_views\": 200000000,\n",
    "    \"video_likes\": 200,\n",
    "    \"video_dislikes\": 200,\n",
    "    \"video_comments\": 10,\n",
    "}\n",
    "youtube_samples = pd.DataFrame([ input ])\n",
    "\n",
    "# Tokenize the title and then compute its embedding:\n",
    "youtube_samples[\"video_title\"] = youtube_samples[\"video_title\"].apply(tokenize)\n",
    "youtube_samples[\"video_title\"] = youtube_samples[\"video_title\"].apply(\n",
    "    average_embedding, word2vec=word2vec, na_vector=mean_title_embedding)\n",
    "\n",
    "#concatenate the views, likes, dislikes, comments\n",
    "youtube_samples = pd.concat(\n",
    "    [\n",
    "        youtube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]],\n",
    "        youtube_samples[\"video_title\"].apply(pd.Series)\n",
    "    ], axis=1)\n",
    "\n",
    "# Compute the log of the video metadata or replace the missing values with the mean values obtained\n",
    "# from the train set:\n",
    "mean_log_video_views = pickle.load(open(\"/content/drive/My Drive/youtube/data/mean-log-video-views\", \"rb\"))\n",
    "mean_log_video_likes = pickle.load(open(\"/content/drive/My Drive/youtube/data/mean-log-video-likes\", \"rb\"))\n",
    "mean_log_video_dislikes = pickle.load(open(\"/content/drive/My Drive/youtube/data/mean-log-video-dislikes\", \"rb\"))\n",
    "mean_log_video_comments = pickle.load(open(\"/content/drive/My Drive/youtube/data/mean-log-video-comments\", \"rb\"))\n",
    "\n",
    "#Data Preprocessing here\n",
    "\n",
    "#Data Transformation\n",
    "youtube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]] = \\\n",
    "    youtube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]].apply(np.log)\n",
    "\n",
    "#If any null itemset then check for them \n",
    "if youtube_samples[\"video_views\"].isnull().any():\n",
    "    youtube_samples[\"video_views\"].fillna(mean_log_video_views, inplace=True)\n",
    "    \n",
    "    \n",
    "if youtube_samples[\"video_likes\"].isnull().any():\n",
    "    youtube_samples[\"video_likes\"].fillna(mean_log_video_likes, inplace=True)\n",
    "    \n",
    "if youtube_samples[\"video_dislikes\"].isnull().any():\n",
    "    youtube_samples[\"video_dislikes\"].fillna(mean_log_video_dislikes, inplace=True)\n",
    "    \n",
    "if youtube_samples[\"video_comments\"].isnull().any():\n",
    "    youtube_samples[\"video_comments\"].fillna(mean_log_video_comments, inplace=True)\n",
    "\n",
    "# Replace any -Inf value with 0:\n",
    "youtube_samples = youtube_samples.replace(-np.inf, 0)\n",
    "\n",
    "# Import the min-max scaler and apply it to the youtube_samples:\n",
    "min_max_scaler = pickle.load(open(\"/content/drive/My Drive/youtube/data/min-max-scaler\", \"rb\"))\n",
    "youtube_samples = pd.DataFrame(min_max_scaler.transform(youtube_samples), columns=youtube_samples.columns)\n",
    "\n",
    "# Import the SVM model:\n",
    "svm = pickle.load(open(\"/content/drive/My Drive/youtube/data/svm\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8538,
     "status": "ok",
     "timestamp": 1543525905046,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "qJK1V7vWGe3K",
    "outputId": "99244e09-cb32-44e3-e3b8-8df4094fefee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Click Bait(1) or Not(0)\n",
    "\n",
    "\n",
    "# Print its prediction:\n",
    "print(svm.predict(youtube_samples)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7OYuPmZT12X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dexters_youtube_clickbait_predict.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
