{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9ZKVFNIGcsk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#for parsing\n",
    "import argparse\n",
    "\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from gensim.parsing.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8544,
     "status": "ok",
     "timestamp": 1543525905043,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "OcW_vD52Gexd",
    "outputId": "de32fb48-5303-44eb-82ac-3518a5073461"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\Abhishek\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#method for tokenization a string in python\n",
    "\n",
    "def tokenize(string):\n",
    "\n",
    "    \"\"\" Tokenizes a string.\n",
    "\n",
    "    Returns a list of stems and, eventually, emojis.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    stop_words = [\n",
    "        \"about\", \"an\", \"are\", \"as\", \"at\", \"be\", \"by\", \"com\", \"for\", \"from\", \"in\", \"is\", \"it\", \"of\",\n",
    "        \"on\", \"or\", \"that\", \"the\", \"this\", \"to\", \"was\", \"what\", \"when\", \"where\", \"who\", \"with\",\n",
    "        \"the\", \"www\"\n",
    "    ]\n",
    "\n",
    "    string = strip_short(\n",
    "        strip_multiple_whitespaces(\n",
    "            strip_punctuation(\n",
    "                split_alphanum(string))),\n",
    "        minsize=2)\n",
    "    \n",
    "    emojis = [c for c in string if c in emoji.UNICODE_EMOJI]\n",
    "    \n",
    "    string = stem_text(re.sub(r\"[^\\w\\s,]\", \"\", string))\n",
    "    \n",
    "    tokens = string.split() + emojis\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "        try:\n",
    "            tokens.remove(stop_word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def average_embedding(tokens, word2vec, na_vector=None):\n",
    "\n",
    "    \"\"\" Embeds a title with the average representation of its tokens.\n",
    "\n",
    "    Returns the mean vector representation of the tokens representations. When no token is in the\n",
    "    \"\"\"\n",
    "\n",
    "    vectors = list()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in word2vec:\n",
    "            vectors.append(word2vec[token])\n",
    "\n",
    "    if len(vectors) == 0 and na_vector is not None:\n",
    "        vectors.append(na_vector)\n",
    "\n",
    "    return np.mean(np.array(vectors), axis=0)\n",
    "\n",
    "\n",
    "word2vec = pickle.load(open(\"data/word2vec\", \"rb\"))\n",
    "mean_title_embedding = pickle.load(open(\"data/mean-title-embedding\", \"rb\"))\n",
    "\n",
    "#User input here:\n",
    "input = {\n",
    "    \"video_title\": \"New York\",\n",
    "    \"video_views\": 20,\n",
    "    \"video_likes\": 200,\n",
    "    \"video_dislikes\": 200,\n",
    "    \"video_comments\": 10,\n",
    "}\n",
    "utube_samples = pd.DataFrame([ input ])\n",
    "\n",
    "utube_samples[\"video_title\"] = utube_samples[\"video_title\"].apply(tokenize)\n",
    "utube_samples[\"video_title\"] = utube_samples[\"video_title\"].apply(\n",
    "    average_embedding, word2vec=word2vec, na_vector=mean_title_embedding)\n",
    "\n",
    "utube_samples = pd.concat(\n",
    "    [\n",
    "        utube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]],\n",
    "        utube_samples[\"video_title\"].apply(pd.Series)\n",
    "    ], axis=1)\n",
    "\n",
    "video_views = pickle.load(open(\"data/mean-log-video-views\", \"rb\"))\n",
    "mean_log_video_likes = pickle.load(open(\"data/mean-log-video-likes\", \"rb\"))\n",
    "video_dislikes = pickle.load(open(\"data/mean-log-video-dislikes\", \"rb\"))\n",
    "video_comments = pickle.load(open(\"data/mean-log-video-comments\", \"rb\"))\n",
    "\n",
    "\n",
    "utube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]] = \\\n",
    "    utube_samples[[\"video_views\", \"video_likes\", \"video_dislikes\", \"video_comments\"]].apply(np.log)\n",
    "\n",
    "if utube_samples[\"video_views\"].isnull().any():\n",
    "    utube_samples[\"video_views\"].fillna(video_views, inplace=True)\n",
    "    \n",
    "    \n",
    "if utube_samples[\"video_likes\"].isnull().any():\n",
    "    utube_samples[\"video_likes\"].fillna(mean_log_video_likes, inplace=True)\n",
    "    \n",
    "if utube_samples[\"video_dislikes\"].isnull().any():\n",
    "    utube_samples[\"video_dislikes\"].fillna(video_dislikes, inplace=True)\n",
    "    \n",
    "if utube_samples[\"video_comments\"].isnull().any():\n",
    "    utube_samples[\"video_comments\"].fillna(video_comments, inplace=True)\n",
    "\n",
    "utube_samples = utube_samples.replace(-np.inf, 0)\n",
    "\n",
    "min_max_scaler = pickle.load(open(\"data/min-max-scaler\", \"rb\"))\n",
    "utube_samples = pd.DataFrame(min_max_scaler.transform(utube_samples), columns=utube_samples.columns)\n",
    "\n",
    "svm = pickle.load(open(\"data/svm\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8538,
     "status": "ok",
     "timestamp": 1543525905046,
     "user": {
      "displayName": "Shared 98999",
      "photoUrl": "",
      "userId": "02966646165121999534"
     },
     "user_tz": 480
    },
    "id": "qJK1V7vWGe3K",
    "outputId": "99244e09-cb32-44e3-e3b8-8df4094fefee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Click Bait(1) or Not(0)\n",
    "\n",
    "print(svm.predict(utube_samples)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7OYuPmZT12X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "dexters_youtube_clickbait_predict.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
